#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ------------------------------------------------------------------------------
# 
# MIT License
# 
# Copyright (c) 2017 Gabriele Girelli
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# 
# ------------------------------------------------------------------------------

# ------------------------------------------------------------------------------
# 
# Author: Gabriele Girelli
# Email: gigi.ga90@gmail.com
# Project: GPSeq
# Description: estimate region centrality from GPSeq sequencing data.
# 
# Changelog:
#   v2.0.0 - first release as a python package.
# 
# ------------------------------------------------------------------------------



# DEPENDENCIES =================================================================

import argparse
from io import StringIO
import numpy as np
import os
import pandas as pd
import pybedtools as pbt
import sys
import tempfile
from tqdm import tqdm

# PARAMETERS ===================================================================

# Add script description
parser = argparse.ArgumentParser(description = """
Description:

 Estimate global centrality. The script performs the following steps:
  (1) Identify & sort chromosomes
  (2) Generate bins
  (3) Group cutsites (intersect)
  (4) Normalize over last condition.
  (5) Prepare domain
  (6) Assign reads to bins (intersect)
  (7) Calculate bin statistics
  (8) Combine condition into a single table
  (9) Estimate centrality
  (10) Rank bins
  (11) Write output

Notes:

 # Cutsite domain --------------------------------------------------------------

  The cutsite domain can be specified as follows:
  1 - all genomic cutsites (universe)
  2 - all cutsites restricted in the experiment (union)
  3 - all cutsites restricted in a condition (separate)
  4 - all cutsites restricted in all conditions (intersection)
  Default is 3 (separate). Also, note that if option 1 is selected, an
  additional argument -l is required.
  
  Statistics (mean, variance) metrics take into account only the cutsites
  included in the specified cutsite domain. The same cutsite domain is used for
  all estimates.
  
  Options 3 and 4 include an empty-cutsites/grups removal step. In this case,
  they are removed before bin assignment, while empty bins are kept. Also,
  normalization is performed after empty bin removal but before bin assignment,
  i.e., either on the grouped or single cutsites.
 
 # Resolution ------------------------------------------------------------------

  Depending on the sequencing resolution, it might not be feasible to go for
  single-cutsite resolution. Thus, cutsite can be grouped for the statistics
  calculation using the -g option.
  
  In case of sub-chromosome bins, the ranking is done in an ordered
  chromosome-wise manner.
 
 # Select specific metrics -----------------------------------------------------

  By default, all the available metrics are calculated. Use -i to provide a list
  of the comma-separated metrics to calculate, while the rest would be excluded.
  Use the -e option to provide a list of the comma-separated metrics not to be
  calculated, while the rest would be included. The available metrics are:

 # -----------------------------------------------------------------------------
""", formatter_class = argparse.RawDescriptionHelpFormatter)

# Positional parameters
parser.add_argument('bedfile', type = str, nargs = '+',
	help = """At least two (2) GPSeq condition bedfiles, in increasing order of
restriction conditions intensity. Expected to be ordered per condition.""")

# Optional parameters
parser.add_argument('-o', '--output', type = str, nargs = 1,
	help = """Path to output folder.""", metavar = "outDir", required = True)
parser.add_argument('-c', '--cutsite-mode', type = int, nargs = 1,
	help = """Custite mode (see Notes).""", choices = range(1, 5),
	metavar = "csMode", default = [3])
csModeLabel = ["1:Universe", "2:Union", "3:Separate+Empty", "4:Intersection"]
parser.add_argument('-l', '--cutsite-bed', type = str, nargs = 1,
	help = """Path to cutsite bedfile. Required for -c1 when -g is not used.""",
	metavar = "csBed", default = [None])
parser.add_argument('-b', '--bin-bed', type = str, nargs = 1,
	help = """Path to bin bedfile. If used, -s and -p are ignored.""",
	metavar = "binBed", default = [None])
parser.add_argument('-s', '--bin-size', type = int, nargs = 1,
	help = """Bin size in bp. Default to chromosome-wide bins.""",
	metavar = "binSize", default = [0])
parser.add_argument('-p', '--bin-step', type = int, nargs = 1,
	help = """Bin step in bp. Default to bin binSize.""",
	metavar = "binStep", default = [0])
parser.add_argument('-g', '--group-size', type = int, nargs = 1,
	help = """Group size in bp. Used to group bins for statistics calculation.
	binSize must be divisible by groupSize. Not used by default.""",
	metavar = "groupSize", default = [0])
metrics = ['prob_2p', 'prob_f', 'prob_g', 'cor_2p', 'cor_f', 'cor_g', 'roc_2p',
	'roc_f', 'roc_g', 'var_2p', 'var_f', 'ff_2p', 'ff_f', 'cv_2p', 'cv_f']
parser.add_argument('-e', '--exclude', type = str, nargs = '*',
	help = """Space-separated list of metrics to exclude from calculation.
	All metrics BUT the specified ones are calculated.""",
	metavar = "metric", default = [None], choices = metrics)
parser.add_argument('-i', '--include', type = str, nargs = '*',
	help = """Space-separated list of metrics to be calculated.
	Only the specified metrics are calculated.""",
	metavar = "metric", default = [None], choices = metrics)
parser.add_argument('-r', '--prefix', type = str, nargs = 1,
	help = """Output name prefix.""", metavar = "prefix", default = [""])
parser.add_argument('-u', '--suffix', type = str, nargs = 1,
	help = """Output name suffix.""", metavar = "suffix", default = [""])
parser.add_argument('-T', type = str, nargs = 1,
	help = '''Path to temporary folder.''', default = [tempfile.gettempdir()])

# Flag parameters
parser.add_argument('-y', '--do-all', action = 'store_const',
	help = """Do not ask for settings confirmation and proceed.""",
	const = True, default = False)
parser.add_argument('-d', '--debug-mode', action = 'store_const',
	help = """Debugging mode: save intermediate results.""",
	const = True, default = False)
parser.add_argument('-n', '--normalize', action = 'store_const',
	help = """Use last condition for normalization.""",
	const = True, default = False)

# Version flag
version = "2.0.0dev"
parser.add_argument('--version', action = 'version',
	version = '%s v%s' % (sys.argv[0], version,))

# Parse arguments
args = parser.parse_args()

# Check input ------------------------------------------------------------------

# At least two bed files if not normalizing
assert_msg = "at least two (2) GPSeq condition bedfiles,"
assert_msg += " in increasing order of restriction"
assert_msg += "intensity are required."
assert 2 <= len(args.bedfile), assert_msg

# At least 3 bed files if normalizing
if args.normalize:
	assert_msg = "at least two (3) GPSeq condition bedfiles,"
	assert_msg += " in increasing order of restriction intensity are"
	assert_msg += " required when normalization is on (-n)."
	assert 3 <= len(args.bedfile), assert_msg

# Bedtools must be installed
assert pbt.check_for_bedtools(), "bedtools required."

# -e and -i cannot be used together
doExclude = not type(None)==type(args.exclude[0])
doInclude = not type(None)==type(args.include[0])
assert not(doExclude and doInclude), "options -e/-i cannot be used together."

# Identify selected metrics
toCalc = metrics
if doInclude: toCalc = args.include[0]
if doExclude: toCalc = [m for m in metrics if m not in args.exclude[0]]

# All provided bed files must exist
for bp in args.bedfile:
	assert os.path.isfile(bp), "file not found: '%s'" % bp

# -l option is mandatory with -gc1
assert_msg = "missing -l option with -gc1."
assertc = 1 == args.cutsite_mode[0] and 0 == args.group_size[0]
assert not(assertc and type(None) == type(args.cutsite_bed[0])), assert_msg

# Bin size, bin step and group size must be positive
assert 0 <= args.bin_size[0], "bin size must be a positive integer."
assert 0 <= args.bin_step[0], "bin step must be a positive integer."
assert 0 <= args.group_size[0], "group size must be a positive integer."

# Bin size >= bin step
assert_msg = "bin size must be greater then or equal to bin step."
assert args.bin_size[0] >= args.bin_step[0], assert_msg

if args.bin_size[0] != 0 and args.bin_step[0] == 0:
	args.bin_step = args.bin_size

if 0 != args.bin_size[0] and 0 != args.group_size[0]:
	assert_msg = "bin size must be divisible by group size."
	assert 0 == args.bin_size[0] % args.group_size[0], assert_msg

if 0 != args.bin_step[0] and 0 != args.group_size:
	assert_msg = "bin step must be greater than group size."
	assert args.bin_step[0] > args.group_size[0], assert_msg

if 0 != args.bin_step[0] and 0 == args.bin_size[0]:
	print("wARNING: missing bin size, ignoring -p option.")
	args.bin_step = [0]

# Temporary folder must exist
assert os.path.isdir(args.T[0]), "temporary folder not found: %s" % args.T[0]
pbt.set_tempdir(args.T[0])

# Adjust prefix/suffix if needed
if '.' != args.prefix[0][-1]: args.prefix[0] += '.'
if '.' != args.suffix[0][0]: args.suffix[0] = '.' + args.suffix[0]

# FUNCTION =====================================================================

def print_settings(args, clear = True):
	'''Show input settings, for confirmation.

	Args:
		args (Namespace): arguments parsed by argparse.
		clear (bool): clear screen before printing.
	'''
	s = " # GPSeq-centrality-estimate\n\n"

	if type(None) != type(args.bin_bed[0]):
		s += " Bin be file: %s\n" % args.bin_bed[0]

	if 0 == args.bin_size[0] and type(None) == type(args.bin_bed[0]):
		s += " Using chr-wide bins.\n"
	else:
		s += "   Bin size : %d\n   Bin step : %d\n" % (
			args.bin_size[0], args.bin_step[0])

	if 0 != args.group_size[0]:
		s += " Group size : %d\n" % args.group_size[0]

	s += "     Domain : %s\n" % csModeLabel[args.cutsite_mode[0] - 1]

	if 1 == args.cutsite_mode[0]:
		s += "    Cutsite : %s\n" % args.cutsite_bed[0]

	if 0 != len(args.prefix[0]): s += " Prefix : %s\n" % args.prefix[0]
	if 0 != len(args.suffix[0]): s += " Suffix : %s\n" % args.suffix[0]

	if args.normalize: s += "\n Normalizing over last condition.\n"
	if args.debug_mode: s += "\n Debugging mode ON.\n"

	doExclude = not type(None)==type(args.exclude[0])
	if doExclude:
		s += "\n Excluded metrics:\n  %s\n" % ", ".join(args.exclude[0])
	doInclude = not type(None)==type(args.include[0])
	if doInclude:
		s += "\n Included metrics:\n  %s\n" % ", ".join(args.include[0])

	s += "\n Output dir : %s\n  Bed files : \n" % args.output[0]
	s += "".join(["   %s\n" % bp for bp in args.bedfile])

	if clear: print("\033[H\033[J%s" % s)
	else: print(s)
	return(s)

def ask(q):
	"""Ask for confirmation. Aborts otherwise.

	Args:
		q (string): question.
	"""
	answer = ''
	while not answer.lower() in ['y', 'n']:
		print("%s %s" % (q, "(y/n)"))
		answer = sys.stdin.readline().strip()

		if 'n' == answer.lower():
			sys.exit("Aborted.\n")
		elif not 'y' == answer.lower():
			print("Please, answer 'y' or 'n'.\n")
	print("")

def chr2chrid(c):
	'''Convert chromosome string to chromosome ID.
	Returns nan for non-standard chromosomes (i.e., not 1-22, X, Y).

	Args:
		c (str): chromosome string.
	'''
	c = c[3:]
	if "X" == c: c = '23'
	if "Y" == c: c = '24'
	if c.isalpha(): return(np.nan)
	else: return(int(c))

def bed_read_or_check(bed):
	'''Checks if a bed file was already parsed, and parses it otherwise.

	Args:
		bed (str): path to bed file, needs to be fully stored in memory.
		bed (pbt.BedTool): parsed bed file content.

	Returns:
		pbt.BedTools: parsed bed file content.
	'''
	if type("") == type(bed):
		assert os.path.isfile(bed), "missing file: %s" % bed
		bed = pbt.BedTool(bed)

	assert_msg = "either path or BedTool() expected."
	assert type(pbt.BedTool()) == type(bed), assert_msg

	return(bed)

def sort_bed_by(bed, fkey = None):
	'''Sort a bed file. By default, sorts by chromosome ID.
	It is possible to provide a custom key function for sorting.

	Args:
		bed (str): path to bed file, needs to be fully stored in memory.
		bed (pbt.BedTool): parsed bed file content.
		fkey (fun): function to extract sorting keys.

	Returns:
		list: list of intervals sorted by key.
	'''
	bed = bed_read_or_check(bed)
	if type(None) == type(fkey): fkey = lambda x: chr2chrid(x)
	return(sorted(bed, key = fkey))

def get_chr_size(bed, d = None):
	'''Extract chromosome size from a bed file.

	Args:
		bed (str): path to bed file, needs to be fully stored in memory.

	Returns:
		dict: (chrom, size) item couples.
	'''
	
	if type("") == type(bed):
		assert os.path.isfile(bed), "missing file: %s" % bed

	if type(None) == type(d): d = {}

	with open(bed, "r+") as IH:
		for line in IH:
			if not line.startswith("track"):
				i = line.strip().split("\t")
				if not i[0] in d.keys():
					d[i[0]] = int(i[2])
				else:
					if int(i[2]) >= d[i[0]]:
						d[i[0]] = int(i[2])

	return(d)

def mk_bed_windows(chr_sizes, bsize, bstep):
	'''Generate sub-chromosome windows.

	Args:
		chr_sizes (dict): (chrom, size) item tuples.
		bsize (int): bin size.
		bstep (int): bin step.

	Returns:
		pbt.BedTools: bin bed.
	'''

	assert bsize >= bstep, "bin size must be greater than or equal to bin step."

	s = ""
	for (c, e) in chr_sizes.items():
		for start in range(0, e, bstep):
			s += "%s\t%d\t%d\t\n" % (c, start, start + bsize)
	return(pbt.BedTool(s, from_string = True))

def bed_to_bins(bins, bed):
	'''Assign regions to bins. Each bin will appear once per each intersecting
	region, with the region value field appended.

	Args:
		bins (pbt.BedTool): bins bed.
		bed (pbt.BedTool): parsed bed.

	Returns:
		pbt.BedTool: grouped bed.
	'''

	# Perform intersection
	isect = bins.intersect(bed, wa = True, wb = True, loj = True)

	# Sum read counts
	d = []
	bi = 1
	with open(isect.fn, "r+") as IH:
		for line in IH:
			i = line.strip().split("\t")
			data = i[:3]
			if float(i[7]) < 0: data.append("0")
			else: data.append(i[7])
			d.append("\t".join(data))

	# Format as bed file
	return(pbt.BedTool("\n".join(d), from_string = True))

def bed_to_combined_bins(bins, bed, fcomb = None):
	'''Groups UMI-uniqued reads from bed file to bins.

	Args:
		bins (pbt.BedTool): bins bed.
		bed (pbt.BedTool): parsed bed.
		fcomb (fun): lambda(x,y) for combining.

	Returns:
		pbt.BedTool: grouped bed.
	'''

	if type(None) == type(fcomb):
		fcomb = lambda x, y: x + y

	# Perform intersection
	isect = bins.intersect(bed, wa = True, wb = True, loj = True)

	# Sum read counts
	d = {}
	bi = 1
	with open(isect.fn, "r+") as IH:
		for line in IH:
			i = line.strip().split("\t")
			i[7] = float(i[7])
			ilabel = " ".join(i[:3])
			if not ilabel in d.keys():
				if i[7] < 0: i[7] = 0
				d[ilabel] = [i[0], int(i[1]), int(i[2]),
					"bin_%d" % bi, i[7]]
				bi += 1
			else:
				d[ilabel][4] = fcomb(d[ilabel][4], i[7])

	# Format as bed file
	s = "\n".join(["%s\t%d\t%d\t%s\t%d" % tuple(v) for v in d.values()])
	return(pbt.BedTool(s, from_string = True))

def norm_bed(normbed, bed):
	'''Normalize one bed over another.
	Discards empty intersections.

	Args:
		normbed (pbt.BedTool): normbed bed.
		bed (pbt.BedTool): parsed bed.

	Returns:
		pbt.BedTool: normalized bed.
	'''
	isect = normbed.intersect(bed, wb = True)
	
	s = ""
	with open(isect.fn, "r+") as IH:
		for line in IH:
			i = line.strip().split("\t")
			if 0 < int(i[4]):
				data = i[5:-1]
				data.append(int(i[9]) / float(i[4]))
				s += "%s\t%s\t%s\t%s\t%.2f\n" % tuple(data)

	return(pbt.BedTool(s, from_string = True))

def mk_cutsite_domain(bedfiles, mode, csbed = None, groups = None):
	'''Build bed file with list of cutsites.
	The cutsite domain can be specified as follows:
	1 - all genomic cutsites (universe)
	2 - all cutsites restricted in the experiment (union)
	3 - all cutsites restricted in a condition (separate)
	4 - all cutsites restricted in all conditions (intersection)
	Default is 3 (separate). Also, note that if option 1 is selected, an
	additional argument -l is required.

	Args:
		bedfiles (list): list of bed files.
		mode (int): custite domain mode (see description).
		csbed (pbt.BedTools): parsed bed with all cutsite in the reference.
		groups (pbt.BedTools): parsed bed with groups, if grouping.
	'''

	assert mode in range(1, 5), "allowed modes: %s" % str(list(range(1, 5)))

	# Identify common cutsites/groups lists for domain preparation -------------

	csbed = None
	if 1 == mode:	# Universe
		if type(None) == type(groups):
			# Use cutsite list
			assert_msg = "cutsite bed required for mode 1."
			assert type("") == type(csbed[0]), assert_msg
			assert os.path.isfile(csbed[0]), "file not found: %s" % csbed[0]
			csbed = pbt.BedTool(csbed[0])
		else:
			# Use groups as cutsites
			assert_msg = "parsed bed tool required, got: %s" % type(groups)
			assert type(pbt.BedTool()) == type(groups), assert_msg
			csbed = groups
	elif 2 == mode:	# Union
		# Identify cutsites from all conditions
		print("> Performing union over beds...")
		csbed = bedfiles[0].cat(*bedfiles[1:], force_truncate = True)
	elif 3 == mode:	# Separated
		pass
	elif 4 == mode:	# Intersection
		print("> Performing intersection over beds...")
		csbed = bedfiles[0]
		for i in range(len(bedfiles[1:])):
			csbed += bedfiles[i]

	# Group domain if needed and save counts -----------------------------------

	if type(None) != type(csbed):
		if type(None) != type(groups):
			# Group cutsites
			csbed = bed_to_combined_bins(groups, csbed)

	return(csbed)

def bed_calc_stats(bed):
	'''Calculate bin-wise statistics.

	Args:
		bed (pbt.BedTool): parsed bed file.

	Returns:
		pd.DataFrame: data frame with bin statistics.
	'''
	
	data = {}
	with open(bed.fn, "r+") as IH:
		for line in IH:
			i = line.strip().split("\t")
			ilabel = "\t".join(i[:3])
			if ilabel in data.keys():
				data[ilabel].append(float(i[3]))
			else:
				data[ilabel] = [float(i[3])]
	
	s = ""
	for (k, v) in data.items():
		s += "%s\t%.2f\t%.2f\t%.2f\t%d\n" % (
			k, sum(v), np.mean(v), np.std(v), len(v))

	df = pd.read_csv(StringIO(s), sep = "\t")
	df.columns = ["chrom", "start", "end", "sum", "mean", "std", "count"]

	return(df)

def calc_p(st, ci):
	'''Calculate restriction probability.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		float
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	row = st.iloc[ci, :]
	return(row['sum'] / (row['cond_nreads'] * row['count']))

def calc_pc(st, ci):
	'''Calculate cumulative restriction probability.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		float
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	if ci == 0: return(calc_p(st, 0))
	else: return(calc_p(st, ci) + calc_pc(st, ci - 1))

def calc_pr(st, ci):
	'''Calculate probability of cumulative restriction.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		float
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	if ci == 0: return(calc_p(st, 0))
	else:
		p = st.ix[:ci, "sum"].sum()
		p /= (st.ix[:ci, "cond_nreads"].sum() * st.ix[0, 'count'])
		return(p)

def calc_var(st, ci):
	'''Calculate variance.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		np.float64
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	return(np.power(st.ix[ci, "std"], 2))

def calc_ff(st, ci):
	'''Calculate Fano Factor.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		np.float64
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	return(np.power(st.ix[ci, "std"], 2) / st.ix[ci, "mean"])

def calc_cv(st, ci):
	'''Calculate coefficient of variation

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		ci (int): condition index.

	Returns:
		np.float64
	'''
	assert ci < st.shape[0], "requested condition (index) not found."
	return(st.ix[ci, "std"] / st.ix[ci, "mean"])

def est_2p(st, f1, f2):
	'''Estimates centrality by combining conditions with two-points fashion.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		f1 (fun): function for calculating condition-wise metrics.
		f2 (fun): function for putting conditions together.

	Returns:
		Estimated centrality.
	'''
	a = f1(st, 0)
	b = f1(st, st.shape[0] - 1)
	return(f2(b, a))

def est_f(st, f1, f2):
	'''Estimates centrality by combining conditions with fixed fashion.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		f1 (fun): function for calculating condition-wise metrics.
		f2 (fun): function for putting conditions together.

	Returns:
		Estimated centrality.
	'''
	out = 0
	a = f1(st, 0)
	for i in range(1, st.shape[0]):
		b = f1(st, i)
		out += f2(b, a)
	return(out)

def est_g(st, f1, f2):
	'''Estimates centrality by combining conditions with global fashion.

	Args:
		st (pd.DataFrame): bin-based data frame (over conditions).
		f1 (fun): function for calculating condition-wise metrics.
		f2 (fun): function for putting conditions together.

	Returns:
		Estimated centrality.
	'''
	out = 0
	a = f1(st, 0)
	for i in range(1, st.shape[0]):
		b = f1(st, i)
		out += f2(b, a)
		a = b
	return(out)

def bin_centrality(df, mlist, progress = True):
	'''Estimate centrality for each bin in a condition combined data frame.

	Args:
		df (pd.DataFrame): multi-condition data frame.
		mlist (list): list of metrics to calculate.
		progress (bool): show progress bar.
	'''

	# Build generator
	indexes = list(set(df.index))
	igen = (i for i in indexes)
	if progress: igen = tqdm(igen, total = len(indexes))

	# Iterate over bins
	odf = []
	for i in igen:
		st = df.loc[i, :]
		st.index = range(st.shape[0])

		# Prepare output
		orow = st.ix[0, ['chrom', 'start', 'end']]

		# Calculate requested metrics
		for m in mlist:
			# Probability
			if m == "prob_2p": # two-points
				orow[m] = est_2p(st, calc_p, lambda x, y: x / y)
			elif m == "prob_f": # fixed
				orow[m] = est_f(st, calc_p, lambda x, y: x / y)
			elif m == "prob_g": # global
				orow[m] = est_g(st, calc_p, lambda x, y: x / y)

			# Cumulative ratio
			elif m == "cor_2p": # two-points
				orow[m] = est_2p(st, calc_pc, lambda x, y: x / y)
			elif m == "cor_f": # fixed
				orow[m] = est_f(st, calc_pc, lambda x, y: x / y)
			elif m == "cor_g": # global
				orow[m] = est_g(st, calc_pc, lambda x, y: x / y)

			# Ratio of cumulative
			elif m == "roc_2p": # two-points
				orow[m] = est_2p(st, calc_pr, lambda x, y: x / y)
			elif m == "roc_f": # fixed
				orow[m] = est_f(st, calc_pr, lambda x, y: x / y)
			elif m == "roc_g": # global
				orow[m] = est_g(st, calc_pr, lambda x, y: x / y)

			# Variance
			elif m == "var_2p": # two-points
				orow[m] = est_2p(st, calc_var, lambda x, y: np.log(x / y))
			elif m == "var_f": # fixed
				orow[m] = est_f(st, calc_var, lambda x, y: np.log(x / y))

			# Fano factor
			elif m == "ff_2p": # two-points
				orow[m] = est_2p(st, calc_ff, lambda x, y: y - x)
			elif m == "ff_f": # fixed
				orow[m] = est_f(st, calc_ff, lambda x, y: y - x)

			# Coefficient of variation
			elif m == "cv_2p": # two-points
				orow[m] = est_2p(st, calc_cv, lambda x, y: y - x)
			elif m == "cv_f": # fixed
				orow[m] = est_f(st, calc_cv, lambda x, y: y - x)

		odf.append(orow)

	# Assemble output
	odf = pd.concat(odf, axis = 1).transpose()
	columns = ['chrom', 'start', 'end']
	columns.extend(mlist)
	odf.columns = columns[:odf.shape[1]]
	odf.index = range(odf.shape[0])

	return(odf)

def rank_centrality(cdf, mlist, progress = True, chrWide = False):
	''''''
	if chrWide:
		labels = [cdf["chrom"].values[i] for i in cdf.index]
	else:
		labels = ["%s:%s-%s" % tuple(cdf.ix[i, :].tolist()[:3])
			for i in cdf.index]
	
	gen = (m for m in mlist if m in cdf.columns)
	if progress: gen = tqdm(gen, total = len(mlist))

	odf = []
	for m in gen:
		odf.append([labels[i] for i in np.argsort(cdf[m].values)])
	
	odf = pd.DataFrame(odf).transpose()
	odf.columns = [m for m in mlist if m in cdf.columns]

	return(odf)

# RUN ==========================================================================

ssettings = print_settings(args)
ask("Confirm settings and proceed?")

# Build run description string
descr = ""
if type(None) != type(args.bin_bed[0]): descr += "customBins"
elif 0 == args.bin_size[0]: descr += "bins.chrWide"
else: descr += "bins.size%d.step%d" % (args.bin_size[0], args.bin_step[0])
if 0 != args.group_size[0]: descr += ".group%d" % args.group_size[0]
descr += ".csm%d" % args.cutsite_mode[0]
if args.normalize: descr += ".norm"

# Save confirmed settings
with open(os.path.join(args.output[0], "%ssettings.%s%s.txt" % (
	args.prefix[0], descr, args.suffix[0])), "w+") as OH:
	OH.write(ssettings)

# Parse all bed files ----------------------------------------------------------

bedfiles = [pbt.BedTool(b) for b in args.bedfile]

# (1) Identify & sort chromosomes ----------------------------------------------
print("Identifying chromosomes...")

chr_sizes = {}
for b in tqdm(args.bedfile):
	chr_sizes = get_chr_size(b, chr_sizes)

# (2) Generate bins ------------------------------------------------------------
print("Generating bins...")

if not type(None) == type(args.bin_bed[0]):
	# Custom bins
	bins = pbt.BedTool(args.bin_bed[0])
elif 0 == args.bin_size[0] and type(None) == type(args.bin_bed[0]):
	# Chromosome-wide bins
	s = "\n".join(["%s\t%d\t%d\t" % (c, 0, e) for (c, e) in chr_sizes.items()])
	bins = pbt.BedTool(s, from_string = True)
else:
	# Sub-chromosome bins
	bins = mk_bed_windows(chr_sizes, args.bin_size[0], args.bin_step[0])

# (3) Group cutsites (intersect) -----------------------------------------------

groups = None
if 0 != args.group_size[0]:
	print("Grouping reads...")

	# Generate groups bed
	groups = mk_bed_windows(chr_sizes, args.group_size[0], args.group_size[0])

	# Intersect
	for i in tqdm(range(len(bedfiles))):
		bedfiles[i] = bed_to_combined_bins(groups, bedfiles[i])

# (4) Normalize over last condition --------------------------------------------

if args.normalize:
	print("Normalizing over last condition...")

	# Identify last condition and remove it from the bed pool
	normbed = bedfiles[-1]
	bedfiles = bedfiles[:-1]

	# Normalize
	for i in tqdm(range(len(bedfiles))):
		bedfiles[i] = norm_bed(normbed, bedfiles[i])

# (5) Prepare domain -----------------------------------------------------------
print("Preparing cutsites...")

csbed = mk_cutsite_domain(bedfiles, args.cutsite_mode[0],
	csbed = args.cutsite_bed, groups = groups)

# (6) Assign reads to bins (intersect) -----------------------------------------
print("Assigning to bins...")

for i in tqdm(range(len(bedfiles))):
	bedfiles[i] = bed_to_bins(bins, bedfiles[i])

# (7) Calculate bin statistics -------------------------------------------------
print("Calculating bin statistics...")

bstats = [bed_calc_stats(b) for b in tqdm(bedfiles)]

# (8) Combine condition into a single table ------------------------------------
print("Combining information...")

# Add condition columns
for i in range(len(bstats)):
	bstats[i]["cond_nreads"] = sum(bstats[i]['count'].values)
	bstats[i]['cond'] = i + 1

# Concatenate
comb = pd.concat(bstats).sort_values(["chrom", "start", "end"])

# Write output
comb.to_csv(os.path.join(args.output[0], "%scombined.%s%s.tsv" % (
	args.prefix[0], descr, args.suffix[0])),
	header = True, index = False, sep = "\t")

# (9) Estimate centrality ------------------------------------------------------
print("Estimating centrality...")

# Estimate centrality of each bin
est = bin_centrality(comb, toCalc)

# Write output
est.to_csv(os.path.join(args.output[0], "%sestimated.%s%s.tsv" % (
	args.prefix[0], descr, args.suffix[0])),
	header = True, index = False, sep = "\t")

# (10) Rank bins ---------------------------------------------------------------
print("Ranking bins...")

# Rank bins
rank = rank_centrality(est, toCalc, chrWide = args.bin_size[0] == 0)

# Write output
rank.to_csv(os.path.join(args.output[0], "%sranked.%s%s.tsv" % (
	args.prefix[0], descr, args.suffix[0])),
	header = True, index = False, sep = "\t")

# End --------------------------------------------------------------------------

print("~ DONE ~")

################################################################################
